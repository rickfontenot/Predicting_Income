simple1.test$income <- as.factor(simple1.test$income)
#Make Predictions and check performance
predprobs<-predict(simple.model1,simple1.test,type="response")
log.roc<-roc(response=simple1.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
auc(log.roc)
#Setup Training set
complex.train <- train_na_as_cat %>% select(features.complex,"income")
complex.train$income <- as.factor(complex.train$income)
#Run Model
complex.model1 <- glm(income ~ .,family="binomial",data=complex.train)
#Setup Test set
complex.test <- test_na_as_cat %>% select(features.complex,"income")
complex.test$income <- as.factor(complex.test$income)
#Make Predictions and check performance
predprobs<-predict(complex.model1,complex.test,type="response")
log.roc<-roc(response=complex.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
auc(log.roc)
predprobs<-predict(complex.model1,complex.test,type="response")
log.roc<-roc(response=complex.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
predprobs
predprobs<-predict(complex.model1,complex.test,type="response")
log.roc<-roc(response=complex.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.9016
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
#Setup Training set
simple1.train <- train_na_as_cat %>% select(original.variables,"marriage_status","collar","income")
simple1.train = simple1.train %>% select(-"workclass",-"marital_status",-"occupation",-"education",-"fnlwgt",-"native_country",-"relationship")
simple1.train$income <- as.factor(simple1.train$income)
#Run Model
simple.model1 <- glm(income ~ .,family="binomial",data=simple1.train)
#Setup Test set
simple1.test <- test_na_as_cat %>% select(original.variables,"marriage_status","collar","income")
simple1.test = simple1.test %>% select(-"workclass",-"marital_status",-"occupation",-"education",-"fnlwgt",-"native_country",-"relationship")
simple1.test$income <- as.factor(simple1.test$income)
#Make Predictions and check performance
predprobs<-predict(simple.model1,simple1.test,type="response")
log.roc<-roc(response=simple1.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.9016
threshold=0.218
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train$income <- as.factor(base.train$income)
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
#Setup Test set
base.test <- test_na_as_cat %>% select(features.complex,"income")
base.test$income <- as.factor(base.test$income)
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train$income <- as.factor(base.train$income)
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
#Setup Test set
base.test <- test_na_as_cat %>% select(original.variables,"income")
base.test$income <- as.factor(base.test$income)
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.9016
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
base.train
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
#Setup Test set
base.test <- test_na_as_cat %>% select(original.variables,"income")
base.test <- mutate_if(base.test, is.character, ~ (as.factor(.x)))
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.9016
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
base.model1
original.variables <- c("age","workclass","fnlwgt","education","education_num","marital_status","occupation","relationship","race","sex","capital_gain","capital_loss","hours_per_week","native_country")
dat.train.x <- train_na_as_cat %>% select(original.variables)
dat.train.x %>% summarise_all(funs(sum(is.na(.))))
dat.train.x <- mutate_if(dat.train.x, is.factor, ~ as.numeric(as.factor(.x)))
dat.train.y <- train_na_as_cat$income
dat.train.y <- as.factor(as.character(dat.train.y))
dat.train.x %>% summarise_all(funs(sum(is.na(.))))
#glmnet requires a matrix
dat.train.x <- data.matrix(dat.train.x)
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response")
#Compare the prediction to the real outcome
head(fit.pred)
head(dat.train.y)
#Create ROC curves
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Get Test Set
dat.val1.x <- test_na_as_cat %>% select(original.variables)
dat.val1.x <- mutate_if(dat.val1.x, is.factor, ~ as.numeric(as.factor(.x)))
dat.val1.x <- data.matrix(dat.val1.x)
dat.val1.y <- test_na_as_cat$income
dat.val1.y <- as.factor(as.character(dat.val1.y))
#Run model from training set on valid set I
fit.pred1 <- predict(cvfit, newx = dat.val1.x, type = "response")
#ROC curves
pred1 <- prediction(fit.pred1[,1], dat.val1.y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
significance_check <- train_na_as_cat %>% select(original.variables,"income")
significance_check$income.binary<-0
significance_check$income.binary[significance_check$income==">50K"] <- 1
significance_check <- significance_check %>% select(-income)
significance_check <- mutate_if(significance_check, is.factor, ~ as.numeric(as.factor(.x)))
significance_check <- mutate_if(significance_check, is.character, ~ as.numeric(as.factor(.x)))
model <- glm(income.binary ~.,family=binomial(link='logit'),data=significance_check)
summary(model)
#occupation and native_country are not significant
significance_check2 <- significance_check %>% select(-fnlwgt)
model2 <- glm(income.binary ~.,family=binomial(link='logit'),data=significance_check2)
summary(model2)
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train <- mutate_if(base.train, is.factor, ~ as.numeric(as.factor(.x)))
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train <- mutate_if(base.train, is.factor, ~ as.numeric(as.factor(.x)))
base.train
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train <- mutate_if(base.train, is.factor, ~ as.numeric(as.factor(.x)))
base.train$income <- base.train$income-1
base.train
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train <- mutate_if(base.train, is.factor, ~ as.numeric(as.factor(.x)))
base.train$income <- base.train$income-1
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
#Setup Test set
base.test <- test_na_as_cat %>% select(original.variables,"income")
base.test <- mutate_if(base.test, is.character, ~ (as.factor(.x)))
base.test <- mutate_if(base.test, is.factor, ~ as.numeric(as.factor(.x)))
base.test$income <- base.test$income-1
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
str(base.test$income)
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,base.test$income)
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,0,1)
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,base.test$income)
log.predclass
base.test$income
log.predclass<-ifelse(predprobs>threshold,0,1)
log.predclass
confusionMatrix(log.predclass,base.test$income)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,0,1)
log.predclass<-factor(log.predclass)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,1,0)
log.predclass<-factor(log.predclass)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best",, main="Main title") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,1,0)
log.predclass<-factor(log.predclass)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
log.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(log.roc,print.thres="best",, main="Base Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,1,0)
log.predclass<-factor(log.predclass)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(simple.model1,simple1.test,type="response")
log.roc<-roc(response=simple1.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best", main="Simple Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.896
threshold=0.218
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
#Setup Training set
complex.train <- train_na_as_cat %>% select(features.complex,"income")
complex.train$income <- as.factor(complex.train$income)
#Run Model
complex.model1 <- glm(income ~ .,family="binomial",data=complex.train)
#Setup Test set
complex.test <- test_na_as_cat %>% select(features.complex,"income")
complex.test$income <- as.factor(complex.test$income)
#Make Predictions and check performance
predprobs<-predict(complex.model1,complex.test,type="response")
log.roc<-roc(response=complex.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(log.roc,print.thres="best", main="Complex Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(log.roc)
#AUC=0.9016
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
standardize = preProcess(train_na_as_cat, method = c("center", "scale"))
standardized.train_na_as_cat = predict(standardize, train_na_as_cat)
standardized.test_na_as_cat = predict(standardize, test_na_as_cat)
#Training Set
dat.train.x <- standardized.train_na_as_cat %>% select(continuous.predictors,"income")
dat.train.x$income <- as.factor(dat.train.x$income)
dat.train.y <- standardized.train_na_as_cat$income
dat.train.y <- as.factor(as.character(dat.train.y))
#Check distributions after standardizing
hist.data.frame(dat.train.x, nclass=20)
#Boxplot capital gain/loss to check for outliers
boxplot(dat.train.x$capital_gain)
boxplot(dat.train.x$capital_loss)
fit.lda <- qda(income ~ ., data = dat.train.x, prior = c(0.76,0.24))
pred.lda <- predict(fit.lda, newdata = dat.train.x)
preds <- pred.lda$posterior
preds <- as.data.frame(preds)
pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Valid set I
dat.val1.x <- standardized.test_na_as_cat %>% select(continuous.predictors)
dat.val1.y <- standardized.test_na_as_cat$income
dat.val1.y <- as.factor(as.character(dat.val1.y))
pred.lda1 <- predict(fit.lda, newdata = dat.val1.x)
preds1 <- pred.lda1$posterior
preds1 <- as.data.frame(preds1)
pred1 <- prediction(preds1[,2],dat.val1.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Re-run with priors info
summary(dat.train.y)
# <=50K = 19799 / 26049 = 0.76
# >50K = 6250 / 26049 = 0.24
#Start here to do LDA ROC plot with best
test.roc <- roc(predictor = preds1[,1], response = dat.val1.y, levels = (levels(dat.val1.y)))
plot(test.roc,print.thres="best",ylim=c(0,1),main="QDA")
text(x = .40, y = .6,paste("AUC-test = ", round(auc.train[[1]],3), sep = ""))
threshold=0.988
log.predclass<-ifelse(preds1[,1]>threshold,"<=50K",">50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,dat.val1.y)
#Setup Training set
base.train <- train_na_as_cat %>% select(original.variables,"income")
base.train <- mutate_if(base.train, is.character, ~ (as.factor(.x)))
base.train <- mutate_if(base.train, is.factor, ~ as.numeric(as.factor(.x)))
base.train$income <- base.train$income-1
#Run Model
base.model1 <- glm(income ~ .,family="binomial",data=base.train)
#Setup Test set
base.test <- test_na_as_cat %>% select(original.variables,"income")
base.test <- mutate_if(base.test, is.character, ~ (as.factor(.x)))
base.test <- mutate_if(base.test, is.factor, ~ as.numeric(as.factor(.x)))
base.test$income <- base.test$income-1
#Make Predictions and check performance
predprobs<-predict(base.model1,base.test,type="response")
base.roc<-roc(response=base.test$income,predictor=predprobs,levels=c(0,1))
plot(base.roc,print.thres="best", main="Base Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(log.roc),3), sep = ""))
auc(base.roc)
#AUC=0.905
threshold=0.214
log.predclass<-ifelse(predprobs>threshold,1,0)
log.predclass<-factor(log.predclass)
confusionMatrix(as.factor(log.predclass),as.factor(base.test$income))
#Make Predictions and check performance
predprobs<-predict(simple.model1,simple1.test,type="response")
simple.roc<-roc(response=simple1.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(simple.roc,print.thres="best", main="Simple Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(simple.roc),3), sep = ""))
auc(simple.roc)
#AUC=0.896
threshold=0.218
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
#Make Predictions and check performance
predprobs<-predict(complex.model1,complex.test,type="response")
complex.roc<-roc(response=complex.test$income,predictor=predprobs,levels=c("<=50K",">50K"))
plot(complex.roc,print.thres="best", main="Complex Logistic Regression") #This graph is nice because the x axis is plotted in terms of specificity rather than FPR
text(x = .40, y = .6,paste("AUC-test = ", round(auc(complex.roc),3), sep = ""))
auc(complex.roc)
#AUC=0.9016
threshold=0.211
log.predclass<-ifelse(predprobs>threshold,">50K","<=50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,complex.test$income)
standardize = preProcess(train_na_as_cat, method = c("center", "scale"))
standardized.train_na_as_cat = predict(standardize, train_na_as_cat)
standardized.test_na_as_cat = predict(standardize, test_na_as_cat)
#Training Set
dat.train.x <- standardized.train_na_as_cat %>% select(continuous.predictors,"income")
dat.train.x$income <- as.factor(dat.train.x$income)
dat.train.y <- standardized.train_na_as_cat$income
dat.train.y <- as.factor(as.character(dat.train.y))
#Check distributions after standardizing
hist.data.frame(dat.train.x, nclass=20)
#Boxplot capital gain/loss to check for outliers
boxplot(dat.train.x$capital_gain)
boxplot(dat.train.x$capital_loss)
fit.lda <- qda(income ~ ., data = dat.train.x, prior = c(0.76,0.24))
pred.lda <- predict(fit.lda, newdata = dat.train.x)
preds <- pred.lda$posterior
preds <- as.data.frame(preds)
pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Valid set I
dat.val1.x <- standardized.test_na_as_cat %>% select(continuous.predictors)
dat.val1.y <- standardized.test_na_as_cat$income
dat.val1.y <- as.factor(as.character(dat.val1.y))
pred.lda1 <- predict(fit.lda, newdata = dat.val1.x)
preds1 <- pred.lda1$posterior
preds1 <- as.data.frame(preds1)
pred1 <- prediction(preds1[,2],dat.val1.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Re-run with priors info
summary(dat.train.y)
# <=50K = 19799 / 26049 = 0.76
# >50K = 6250 / 26049 = 0.24
#Start here to do LDA ROC plot with best
qda.roc <- roc(predictor = preds1[,1], response = dat.val1.y, levels = (levels(dat.val1.y)))
plot(qda.roc,print.thres="best",ylim=c(0,1),main="QDA")
text(x = .40, y = .6,paste("AUC-test = ", round(auc.train[[1]],3), sep = ""))
threshold=0.988
log.predclass<-ifelse(preds1[,1]>threshold,"<=50K",">50K")
log.predclass<-factor(log.predclass)
confusionMatrix(log.predclass,dat.val1.y)
#without color for cutoff; but adding colors to allow for comarisons of the curves
plot(base.roc)
plot(simple.roc,col="orange", add = TRUE)
plot(complex.roc,col="blue", add = TRUE)
plot(qda.roc,col="red", add = TRUE)
legend("bottomright",legend=c("Base","Simple","Complex","QDA"),col=c("black","orange","blue","red"),lty=1,lwd=1)
abline(a=0, b= 1)
#without color for cutoff; but adding colors to allow for comarisons of the curves
plot(base.roc)
plot(simple.roc,col="orange", add = TRUE)
plot(complex.roc,col="blue", add = TRUE)
plot(qda.roc,col="red", add = TRUE)
legend("bottomright",legend=c("Base","Simple","Complex","QDA"),col=c("black","orange","blue","red"),lty=1,lwd=1)
#without color for cutoff; but adding colors to allow for comarisons of the curves
plot(base.roc)
plot(simple.roc,col="orange", add = TRUE)
plot(complex.roc,col="blue", add = TRUE)
plot(qda.roc,col="red", add = TRUE,main="ROC Comparison by Model")
legend("bottomright",legend=c("Base","Simple","Complex","QDA"),col=c("black","orange","blue","red"),lty=1,lwd=1)
#without color for cutoff; but adding colors to allow for comarisons of the curves
plot(base.roc,main="ROC Comparison by Model")
plot(simple.roc,col="orange", add = TRUE)
plot(complex.roc,col="blue", add = TRUE)
plot(qda.roc,col="red", add = TRUE)
legend("bottomright",legend=c("Base","Simple","Complex","QDA"),col=c("black","orange","blue","red"),lty=1,lwd=1)
ggroc(base.roc)
ggroc(list(s100b=base.roc, wfns=simple.roc))
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","orange", "blue", "red"))
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red"))
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red"))+ labs(fill = "Model")
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red")) + labs(x = "Specificity", y = "Sensitivity", linetype="Model")
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red")) + labs(x = "Specificity", y = "Sensitivity", linetype="Model")+
geom_abline()
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + theme_classic() + ggtitle("The new title") + labs(x = "1 - Specificity",y = "Sensitivity",linetype = "Different legend title")
roclist <- list("Base" = base.roc,"Simple" = simple.roc,"Complex" = complex.roc, "QDA" = qda.roc)
g <- ggroc(roclist, aes = "linetype", legacy.axes = TRUE) +
geom_abline() +
theme_classic() +
ggtitle("The new title") +
labs(x = "1 - Specificity",
y = "Sensitivity",
linetype = "Different legend title")
g
ggroc(roclist, aes = "linetype", legacy.axes = TRUE) + geom_abline() + theme_classic() + ggtitle("The new title") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model")
ggroc(roclist, aes = "linetype", legacy.axes = TRUE) + geom_abline() + scale_colour_manual(values = c("black","green", "blue", "red")) + theme_classic() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model")
ggroc(roclist, aes = "linetype", legacy.axes = TRUE) + geom_abline() + theme_classic() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model") + scale_colour_manual(values = c("black","green", "blue", "red"))
ggroc(roclist, aes = "linetype", legacy.axes = TRUE) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model") + scale_colour_manual(values = c("black","green", "blue", "red"))
ggroc(roclist, aes = "linetype", legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model")
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model")
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity", linetype = "Model") +
theme(legend.title=element_blank())
ggroc(list(Base=base.roc, Simple=simple.roc,Complex=complex.roc, QDA=qda.roc))+ scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + theme_classic() + ggtitle("The new title") + labs(x = "1 - Specificity",y = "Sensitivity",linetype = "Different legend title")+ theme(legend.position=c(.8,.2))
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity") + theme(legend.title=element_blank())+ theme(legend.position=c(.8,.2))
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity") + theme(legend.title=element_blank())
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity") + theme(legend.title=element_blank())+ coord_fixed()
ggroc(roclist, legacy.axes = TRUE) + scale_colour_manual(values = c("black","green", "blue", "red")) + geom_abline() + ggtitle("ROC Comparison by Model") + labs(x = "Specificity", y = "Sensitivity") + theme(legend.title=element_blank())+ coord_fixed() + theme(plot.title = element_text(hjust = 0.5))
